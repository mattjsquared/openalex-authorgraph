{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ab34e0-c1ff-41e0-8d74-e467605c0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests, csv, time, copy, warnings\n",
    "\n",
    "from functools import partial as functools_partial\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7b53f0-212e-4e13-ad37-5ffde4100dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.openalex.org/\"\n",
    "USER_EMAIL = \"matthew_jones@brown.edu\" #used only for access to OpenAlex's \"polite pool\" (more reliable data rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e20e436-7e9e-4dc9-8eeb-85aade2942ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers: HTTP GET with exponential backoff on \"429\" errors & automated GET pagination ---\n",
    "@contextmanager\n",
    "def _conditional_session(\n",
    "  session: requests.Session = None\n",
    ") -> requests.Session:\n",
    "  \"\"\"\n",
    "  Context‐manager that yields a requests.Session for HTTP requests.\n",
    "\n",
    "  If an existing Session is provided, that session is reused (and not closed).\n",
    "  Otherwise, a new Session is created at entry and automatically closed on exit.\n",
    "\n",
    "  Args:\n",
    "    session (requests.Session, optional): An existing requests Session to reuse. \n",
    "      If None, a new Session will be created.\n",
    "\n",
    "  Yields:\n",
    "    requests.Session: The Session object to use within the context.\n",
    "\n",
    "  Example:\n",
    "    with _conditional_session() as sess:\n",
    "      resp = sess.get(url)\n",
    "  \"\"\"\n",
    "  if session is None:\n",
    "    s = requests.Session()\n",
    "    try:\n",
    "      yield s\n",
    "    finally:\n",
    "      s.close()\n",
    "  else:\n",
    "    # assume user wants to manage its lifetime\n",
    "    yield session\n",
    "\n",
    "\n",
    "def request_with_backoff(\n",
    "  url: str,\n",
    "  params: dict = None,\n",
    "  session: requests.Session = None,\n",
    "  max_retries: int = 10,\n",
    "  max_wait: float = 10\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Perform an HTTP GET with automatic retry logic for rate‐limit and occasional 404 fallbacks.\n",
    "\n",
    "  This function will:\n",
    "    1. Attach a `mailto` parameter for polite API usage.\n",
    "    2. On HTTP 404 responses against an `/authors` endpoint, retry once against `/people` alias.\n",
    "    3. On HTTP 429 responses (rate limit), retry with exponential backoff until\n",
    "       either `max_retries` is reached or `max_wait` seconds have elapsed.\n",
    "    4. Raise for any other HTTP error status.\n",
    "\n",
    "  Args:\n",
    "    url (str): Full OpenAlex API URL (e.g. \"https://api.openalex.org/works\").\n",
    "    params (dict, optional): Query parameters to include; a `mailto` key will be added.\n",
    "    session (requests.Session, optional): Existing Session to reuse; if None, a new one is created.\n",
    "    max_retries (int): Max number of 429‐retry attempts.\n",
    "    max_wait (float): Max total time (in seconds) to spend retrying before giving up.\n",
    "\n",
    "  Returns:\n",
    "    dict: The JSON‐decoded response body.\n",
    "\n",
    "  Raises:\n",
    "    requests.exceptions.HTTPError: If a non‐retryable error occurs (anything other than handled 404/429),\n",
    "      or if 429 retries/time‐budget are exhausted.\n",
    "  \"\"\"\n",
    "  retries = 0\n",
    "  pause = 0.1\n",
    "  if params is None:\n",
    "    params = {}\n",
    "  params.update({'mailto': USER_EMAIL})\n",
    "  with _conditional_session(session) as _session:\n",
    "    tstart = time.time()\n",
    "    while True:\n",
    "      r = _session.get(url, params=params)\n",
    "      # If searching for authors and we hit a 404, retry with the \"people\" url\n",
    "      if (r.status_code == 404) and ('authors' in url):\n",
    "        url = f\"{BASE_URL}people\"\n",
    "        continue\n",
    "      # If we hit a rate limit and still can retry within time budget\n",
    "      if (r.status_code == 429) and (retries < max_retries) and ((time.time() - tstart) <= max_wait):\n",
    "        time.sleep(pause)\n",
    "        retries += 1\n",
    "        pause *= 2  # exponential increase\n",
    "        continue\n",
    "      # Either success or non-retryable error\n",
    "      r.raise_for_status()\n",
    "      break\n",
    "  return r.json()\n",
    "\n",
    "\n",
    "def _paginate_request(\n",
    "  url: str,\n",
    "  params: dict = None,\n",
    "  session: requests.Session = None,\n",
    "  per_page: int = 200,\n",
    "  max_pages: int = None\n",
    "):\n",
    "  \"\"\"\n",
    "  Lazily retrieve paginated results from an OpenAlex endpoint using cursor‐based pagination.\n",
    "\n",
    "  This generator will:\n",
    "    1. Initialize the cursor to \"*\" and include a per‐page limit.\n",
    "    2. Repeatedly call `request_with_backoff()` to fetch each page.\n",
    "    3. Yield the full JSON response for each page (including both \"results\" and \"meta\").\n",
    "    4. Stop when the returned page has no `meta.next_cursor` or when `max_pages` is reached.\n",
    "\n",
    "  Args:\n",
    "    url (str): Full API endpoint URL (e.g. \"https://api.openalex.org/works\").\n",
    "    params (dict, optional): Base query parameters; will be copied and augmented.\n",
    "    session (requests.Session, optional): Session for HTTP calls; if None, one is created.\n",
    "    per_page (int): Number of items to fetch per page (added as \"per-page\" param).\n",
    "    max_pages (int, optional): Maximum number of pages to retrieve; if None, fetch until exhausted.\n",
    "\n",
    "  Yields:\n",
    "    dict: Each page’s parsed JSON payload, containing:\n",
    "      - \"results\": list of entity records for that page\n",
    "      - \"meta\":   metadata including \"count\", \"next_cursor\", etc.\n",
    "  \"\"\"\n",
    "  count = 0\n",
    "  _params = (params or {}).copy()\n",
    "  _params.update({\"per-page\": per_page, \"cursor\": \"*\"})\n",
    "  with _conditional_session(session) as _session:\n",
    "    while True:\n",
    "      page = request_with_backoff(url, params=_params, session=_session)\n",
    "      yield page\n",
    "      next_cursor = page.get(\"meta\", {}).get(\"next_cursor\")\n",
    "      count += 1\n",
    "      if not next_cursor or (max_pages is not None and count >= max_pages):\n",
    "        break\n",
    "      _params[\"cursor\"] = next_cursor\n",
    "\n",
    "\n",
    "def request_collated_pages(\n",
    "  url: str,\n",
    "  params: dict = None,\n",
    "  session: requests.Session = None,\n",
    "  per_page: int = 200,\n",
    "  max_pages: int = None\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Retrieve all pages from an OpenAlex endpoint and group their payloads.\n",
    "\n",
    "  This function will:\n",
    "    1. Use the cursor‑based pagination generator `_paginate_request` to fetch each page.\n",
    "    2. Collect each page’s “results” lists into a single list of lists.\n",
    "    3. Collect each page’s “meta” dictionaries into a single list of page metadata.\n",
    "\n",
    "  Args:\n",
    "    url (str): The base OpenAlex API URL to page through (e.g. \"https://api.openalex.org/works\").\n",
    "    params (dict, optional): Query parameters to include on every request (augmented per page).\n",
    "    session (requests.Session, optional): An existing Session to reuse for HTTP calls; if None, a new Session is created.\n",
    "    per_page (int): Number of items to request per page.\n",
    "    max_pages (int, optional): If set, limits the number of pages fetched; if None, fetches until no more pages.\n",
    "\n",
    "  Returns:\n",
    "    dict:\n",
    "      - \"results\" (List[List[dict]]): A list where each element is the `\"results\"` list from one page.\n",
    "      - \"meta\"    (List[dict])      : A list of the `\"meta\"` dict for each fetched page.\n",
    "\n",
    "  Example:\n",
    "    pages = request_collated_pages(\"https://api.openalex.org/works\", params={\"filter\": \"author.id:A123\"}, per_page=100)\n",
    "    all_results = pages[\"results\"]        # [[...page1...], [...page2...], ...]\n",
    "    all_meta    = pages[\"meta\"]           # [{...meta1...}, {...meta2...}, ...]\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    pages = list(_paginate_request(url, params=params, session=_session, per_page=per_page, max_pages=max_pages))\n",
    "  collated = {\n",
    "    \"results\": [p[\"results\"] for p in pages],\n",
    "    \"meta\":    [p[\"meta\"]    for p in pages],\n",
    "  }\n",
    "  return collated\n",
    "\n",
    "\n",
    "def request_entities(\n",
    "  url: str,\n",
    "  params: dict = None,\n",
    "  session: requests.Session = None,\n",
    "  per_page: int = 200,\n",
    "  max_pages: int = None,\n",
    "  include_meta: bool = False\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Retrieve all records from a paginated OpenAlex endpoint, optionally summarizing pagination metadata.\n",
    "\n",
    "  This function will:\n",
    "    1. Fetch every page of results via `request_collated_pages`.\n",
    "    2. Flatten the per‑page “results” lists into a single list.\n",
    "    3. If `include_meta` is True, aggregate shared metadata across pages.\n",
    "\n",
    "  Args:\n",
    "    url (str): Base API endpoint (e.g. \"https://api.openalex.org/works\").\n",
    "    params (dict, optional): Query parameters applied to every page request.\n",
    "    session (requests.Session, optional): Session to reuse for HTTP requests; if None, a new one is created.\n",
    "    per_page (int): Number of items to request per page.\n",
    "    max_pages (int, optional): Maximum number of pages to fetch; if None, continues until no more pages.\n",
    "    include_meta (bool): If True, include aggregate metadata under the “meta” key.\n",
    "\n",
    "  Returns:\n",
    "    dict:\n",
    "      - \"results\" (List[dict]): All items from every page, concatenated.\n",
    "      - \"meta\" (dict, optional): Present only if `include_meta` is True, containing:\n",
    "          • \"count\":             Total items available (from first page’s metadata).\n",
    "          • \"groups_count\":      Optional grouping count (from first page’s metadata).\n",
    "          • \"page_count\":        Number of pages retrieved.\n",
    "          • \"db_response_time_total\": Sum of `db_response_time_ms` across all pages.\n",
    "\n",
    "  Example:\n",
    "    resp = request_entities(\n",
    "      \"https://api.openalex.org/works\",\n",
    "      params={\"filter\": \"author.id:A123\"},\n",
    "      include_meta=True\n",
    "    )\n",
    "    all_works = resp[\"results\"]\n",
    "    info = resp[\"meta\"]\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    pages = request_collated_pages(url, params=params, session=_session, per_page=per_page, max_pages=max_pages)\n",
    "  # Flatten only the per‐page result lists, not the dict keys:\n",
    "  flat = [item for page_results in pages[\"results\"] for item in page_results]\n",
    "  out = {\"results\": flat}\n",
    "  if include_meta:\n",
    "    first_meta = pages[\"meta\"][0]\n",
    "    page_count = len(pages[\"meta\"])\n",
    "    total_db = sum(m.get(\"db_response_time_ms\", 0) for m in pages[\"meta\"])\n",
    "    out[\"meta\"] = {\n",
    "      \"count\": first_meta.get(\"count\"),\n",
    "      \"groups_count\": first_meta.get(\"groups_count\"),\n",
    "      \"page_count\": page_count,\n",
    "      \"db_response_time_total\": total_db,\n",
    "      }\n",
    "  return out\n",
    "\n",
    "\n",
    "# --- Field filtering utility ---\n",
    "def _filter_fields(\n",
    "  obj: dict,\n",
    "  fields\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Reduce a JSON object to only the specified top-level fields.\n",
    "\n",
    "  Args:\n",
    "    obj (dict): The original JSON object.\n",
    "    fields (str or list of str, optional): Field names to keep.\n",
    "      If None or empty, returns the original object.\n",
    "\n",
    "  Returns:\n",
    "    dict: Filtered JSON containing only requested fields.\n",
    "  \"\"\"\n",
    "  if not fields:\n",
    "    return obj\n",
    "  # Normalize single string to list\n",
    "  if isinstance(fields, str):\n",
    "    fields = [fields]\n",
    "  return {f: obj.get(f) for f in fields}\n",
    "\n",
    "\n",
    "def _fields2params(\n",
    "  fields\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Format OpenAlex field filter(s) as HTTP GET parameters.\n",
    "\n",
    "  Args:\n",
    "    fields (str or list of str, optional): Field names to keep.\n",
    "      If None or empty, returns empty parameters.\n",
    "\n",
    "  Returns:\n",
    "    dict: `fields` formatted for HTTP GET request.\n",
    "  \"\"\"\n",
    "  params = {}\n",
    "  if (fields is None) or (len(fields) == 0):\n",
    "    return params\n",
    "  if isinstance(fields, str):\n",
    "    params['select'] = fields if (',' in fields) else [fields]\n",
    "  else:\n",
    "    params['select'] = ','.join(fields)\n",
    "  return params\n",
    "  \n",
    "\n",
    "\n",
    "# --- ID normalization ---\n",
    "def standardize_author_ids(\n",
    "  authors,\n",
    "  session: requests.Session = None\n",
    ") -> list:\n",
    "  \"\"\"\n",
    "  Convert a mix of ORCID identifiers, OpenAlex author IDs/URLs, or author dicts\n",
    "  into a list of canonical OpenAlex author URLs.\n",
    "\n",
    "  This function will:\n",
    "    1. Accept a single author (str or dict) or a list of them.\n",
    "    2. Identify which inputs look like ORCIDs (contain exactly 3 hyphens).\n",
    "    3. Batch‑fetch the corresponding OpenAlex author URLs for those ORCIDs.\n",
    "    4. Preserve any inputs that are already OpenAlex URLs/IDs unchanged.\n",
    "    5. Return a list of OpenAlex author URLs in the same order as the inputs.\n",
    "\n",
    "  Args:\n",
    "    authors (str, dict, or list[str or dict]):\n",
    "      - If dict, must contain an 'id' key with the OpenAlex URL.\n",
    "      - If str, may be an ORCID (e.g. \"0000-0002-1234-5678\"), an ORCID URL,\n",
    "        or an OpenAlex author URL/ID (e.g. \"A123456\").\n",
    "      - May also be a list of such values.\n",
    "    session (requests.Session, optional):\n",
    "      An existing HTTP session to reuse for the batch lookup. If None,\n",
    "      a temporary session is created and closed automatically.\n",
    "\n",
    "  Returns:\n",
    "    list[str]: Canonical OpenAlex author URLs, in the same order as the inputs.\n",
    "\n",
    "  Example:\n",
    "    # Single ORCID string\n",
    "    standardize_author_ids(\"0000-0002-3318-5801\")\n",
    "    # -> [\"https://openalex.org/A5040982088\"]\n",
    "\n",
    "    # Mixed list of ORCID and OpenAlex IDs\n",
    "    standardize_author_ids([\n",
    "      \"0000-0002-3318-5801\",\n",
    "      \"https://openalex.org/A123456\"\n",
    "    ])\n",
    "    # -> [\"https://openalex.org/A5040982088\", \"https://openalex.org/A123456\"]\n",
    "  \"\"\"\n",
    "  # normalize to a list of strings\n",
    "  if isinstance(authors, dict):\n",
    "    return [authors['id']]\n",
    "  if isinstance(authors, str):\n",
    "    authors = [authors]\n",
    "  # extract the bare tokens (either ORCID strings or OA IDs)\n",
    "  tokens = [a.rstrip('/').split('/')[-1] for a in authors]\n",
    "  # pick out just the ORCID‐looking tokens (4 hyphens)\n",
    "  is_orcid = lambda t: t.count('-') == 3\n",
    "  orcids = [t for t in tokens if is_orcid(t)]\n",
    "  # batch‐lookup the ORCIDs in one call\n",
    "  mapping = {}\n",
    "  if orcids:\n",
    "    params = _fields2params(['orcid', 'id'])\n",
    "    params['filter'] = 'orcid:' + '|'.join(orcids)\n",
    "    with _conditional_session(session) as _session:\n",
    "      r = request_entities(f\"{BASE_URL}authors\", params=params, session=_session)\n",
    "    # build quick lookup from short‐form ORCID → full OA URL\n",
    "    mapping = {auth['orcid'].split('/')[-1]: auth['id'] for auth in r.get('results', [])}\n",
    "  # rebuild preserving order; non-ORCID tokens stay as originally given\n",
    "  result = [mapping.get(tok, auth) for tok, auth in zip(tokens, authors)]\n",
    "  return result\n",
    "\n",
    "\n",
    "def standardize_work_ids(\n",
    "  works,\n",
    "  session: requests.Session = None\n",
    ") -> list:\n",
    "  \"\"\"\n",
    "  Convert a mix of DOIs, OpenAlex work IDs/URLs, or work dicts into canonical OpenAlex work URLs.\n",
    "\n",
    "  This function will:\n",
    "    1. Accept a single work identifier (str or dict) or a list of them.\n",
    "    2. Identify which inputs look like DOIs (the part after “.org/” starts with \"10.\").\n",
    "    3. Batch‑fetch the corresponding OpenAlex work URLs for those DOIs in a single API call.\n",
    "    4. Preserve any inputs that are already OpenAlex work URLs/IDs unchanged.\n",
    "    5. Return a list of OpenAlex work URLs in the same order as the inputs.\n",
    "\n",
    "  Args:\n",
    "    works (str, dict, or list[str or dict]):\n",
    "      - If dict, must contain an 'id' key with the OpenAlex URL.\n",
    "      - If str, may be a DOI (e.g. \"10.7717/peerj.4375\"), a DOI URL\n",
    "        (e.g. \"https://doi.org/10.7717/peerj.4375\"), or an OpenAlex work URL/ID\n",
    "        (e.g. \"W123456\" or \"https://openalex.org/W123456\").\n",
    "      - May also be a list of such values.\n",
    "    session (requests.Session, optional):\n",
    "      An existing HTTP session to reuse for the batch lookup. If None,\n",
    "      a temporary session is created and closed automatically.\n",
    "\n",
    "  Returns:\n",
    "    list[str]: Canonical OpenAlex work URLs, in the same order as the inputs.\n",
    "\n",
    "  Examples:\n",
    "    # Single DOI string\n",
    "    standardize_work_ids(\"10.7717/peerj.4375\")\n",
    "    # -> [\"https://openalex.org/W4375\"]\n",
    "\n",
    "    # Mixed list of DOI and OpenAlex IDs\n",
    "    standardize_work_ids([\n",
    "      \"10.7717/peerj.4375\",\n",
    "      \"https://openalex.org/W123456\"\n",
    "    ])\n",
    "    # -> [\"https://openalex.org/W4375\", \"https://openalex.org/W123456\"]\n",
    "  \"\"\"\n",
    "  # normalize to a list of strings\n",
    "  if isinstance(works, dict):\n",
    "    return [works['id']]\n",
    "  if isinstance(works, str):\n",
    "    works = [works]\n",
    "  # extract the bare tokens (either DOIs or OA IDs)\n",
    "  tokens = [w for w in works]\n",
    "  # pick out just the DOI‐looking tokens\n",
    "  is_doi = lambda t: t.rstrip('/').split('.org/')[-1].startswith('10.')\n",
    "  dois = [t for t in tokens if is_doi(t)]\n",
    "  # batch‐lookup the DOIs in one call\n",
    "  mapping = {}\n",
    "  if dois:\n",
    "    params = _fields2params(['doi', 'id'])\n",
    "    params['filter'] = 'doi:' + '|'.join(dois)\n",
    "    with _conditional_session(session) as _session:\n",
    "      r = request_entities(f\"{BASE_URL}works\", params=params, session=_session)\n",
    "    # build quick lookup from short‐form DOI → full OA URL\n",
    "    mapping = {wk['doi']: wk['id'] for wk in r.get('results', []) if wk.get('doi')}\n",
    "  # rebuild preserving order; non-ORCID tokens stay as originally given\n",
    "  result = [mapping.get(tok, wk) for tok, wk in zip(tokens, works)]\n",
    "  return result\n",
    "\n",
    "\n",
    "# --- Direct Entity Fetchers ---\n",
    "def fetch_authors(\n",
    "  author_ids,\n",
    "  fields = None,\n",
    "  session: requests.Session = None\n",
    ") -> list:\n",
    "  \"\"\"\n",
    "  Retrieve one or more OpenAlex author records in a single batched API call.\n",
    "\n",
    "  This function accepts a single author identifier or a list of them, where each identifier\n",
    "  can be:\n",
    "    - An OpenAlex author URL (e.g. \"https://openalex.org/A123456\")\n",
    "    - An OpenAlex author ID (e.g. \"A123456\")\n",
    "    - An ORCID (e.g. \"0000-0002-3318-5801\") or ORCID URL (e.g. \"https://orcid.org/0000-0002-3318-5801\")\n",
    "    - A dict representing a partial author JSON with at least an \"id\" key\n",
    "\n",
    "  The identifiers are first normalized to canonical OpenAlex URLs, then fetched\n",
    "  in one request using the `ids.openalex` filter. The returned list is ordered\n",
    "  to correspond exactly to the order of the input identifiers.\n",
    "\n",
    "  Args:\n",
    "    author_ids (str, dict, or list[str or dict]):\n",
    "      Single author identifier or list of identifiers/dicts as described above.\n",
    "    fields (str or list[str], optional):\n",
    "      Fields to include in each author record (returned under `\"select\"`). If None,\n",
    "      all top‑level fields are returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing HTTP session to reuse for all requests. If None, a temporary session\n",
    "      will be created and closed automatically.\n",
    "\n",
    "  Returns:\n",
    "    list[dict]:\n",
    "      A list of author JSON objects, in the same order as the provided `author_ids`.\n",
    "\n",
    "  Raises:\n",
    "    RuntimeError:\n",
    "      If any requested author ID could not be retrieved (i.e., the API did not return it).\n",
    "  \"\"\"\n",
    "  params = _fields2params(fields)\n",
    "  params.update({'per-page': 200, 'cursor': '*'})\n",
    "  url = f\"{BASE_URL}authors\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_ids = standardize_author_ids(author_ids, session=_session)\n",
    "    params['filter'] = f\"ids.openalex:{'|'.join(author_ids)}\"\n",
    "    data = request_entities(url, params=params, session=_session)\n",
    "  data = data['results']\n",
    "  author_map = {auth['id']: auth for auth in data}\n",
    "  data = [author_map.get(aid) for aid in author_ids]\n",
    "  if any(auth is None for auth in data):\n",
    "    missing_id = [aid for aid, auth in zip(author_ids, data) if auth is None]\n",
    "    raise RuntimeError(\"≥1 author was not retrieved:\\n\"+\"\\n\".join(f\"{mid}\" for mid in missing_id))\n",
    "  return data\n",
    "\n",
    "\n",
    "def fetch_works(\n",
    "  work_ids,\n",
    "  fields = None,\n",
    "  session: requests.Session = None\n",
    ") -> list:\n",
    "  \"\"\"\n",
    "  Retrieve one or more OpenAlex work records in a single batched API call.\n",
    "\n",
    "  This function accepts a single work identifier or a list of them, where each identifier\n",
    "  can be:\n",
    "    - An OpenAlex work URL (e.g. \"https://openalex.org/W123456\")\n",
    "    - An OpenAlex work ID (e.g. \"W123456\")\n",
    "    - A DOI string (e.g. \"10.7717/peerj.4375\") or DOI URL\n",
    "      (e.g. \"https://doi.org/10.7717/peerj.4375\")\n",
    "    - A dict containing at least an \"id\" key (and optionally \"doi\")\n",
    "\n",
    "  All identifiers are first normalized to canonical OpenAlex work URLs via\n",
    "  `standardize_work_ids()`, then fetched in one API call using the `ids.openalex`\n",
    "  filter. The returned list is ordered to correspond exactly to the order of the\n",
    "  input identifiers.\n",
    "\n",
    "  Args:\n",
    "    work_ids (str, dict, or list[str or dict]):\n",
    "      Single work identifier or list of identifiers/dicts as described above.\n",
    "    fields (str or list[str], optional):\n",
    "      Fields to include in each work record (mapped to the \"select\" parameter).\n",
    "      If None, all top‑level fields are returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing HTTP session to reuse for all requests. If None, a temporary session\n",
    "      will be created and closed automatically.\n",
    "\n",
    "  Returns:\n",
    "    list[dict]:\n",
    "      A list of work JSON objects, in the same order as the provided `work_ids`.\n",
    "\n",
    "  Raises:\n",
    "    RuntimeError:\n",
    "      If any requested work ID could not be retrieved (i.e., the API did not return\n",
    "      a record for one or more of the input identifiers).\n",
    "  \"\"\"\n",
    "  params = _fields2params(fields)\n",
    "  url = f\"{BASE_URL}works\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    work_ids = standardize_work_ids(work_ids)\n",
    "    params['filter'] = f\"ids.openalex:{'|'.join(work_ids)}\"\n",
    "    data = request_entities(url, params=params, session=_session)\n",
    "  data = data['results']\n",
    "  work_map = {wk['id']: wk for wk in data}\n",
    "  data = [work_map.get(wid) for wid in work_ids]\n",
    "  if any(wk is None for wk in data):\n",
    "    missing = [wid for wid, wk in zip(work_ids, data) if wk is None]\n",
    "    raise RuntimeError(\"≥1 work was not retrieved:\\n\"+\"\\n\".join(f\"{msng}\" for msng in missing))\n",
    "  return data\n",
    "\n",
    "\n",
    "def fetch_works_from_author(\n",
    "  author_id,\n",
    "  fields = None,\n",
    "  session: requests.Session = None\n",
    ") -> list:\n",
    "  \"\"\"\n",
    "  Retrieve every work record for a specified OpenAlex author in a single batched request.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes the input `author_id` (which may be an ORCID, OpenAlex ID, URL, or dict)\n",
    "       into the canonical OpenAlex author URL.\n",
    "    2. Uses the OpenAlex `/works` endpoint with an `author.id:` filter to page through\n",
    "       all works authored by that individual.\n",
    "    3. Returns the complete list of work JSON objects, optionally filtered to only\n",
    "       include the requested fields.\n",
    "\n",
    "  Args:\n",
    "    author_id (str or dict):\n",
    "      An author identifier, which may be:\n",
    "        - An OpenAlex author URL (e.g. \"https://openalex.org/A123456\")\n",
    "        - An OpenAlex author ID (e.g. \"A123456\")\n",
    "        - An ORCID URL or code (e.g. \"https://orcid.org/0000-0001-2345-6789\" or \"0000-0001-2345-6789\")\n",
    "        - A dict containing at least an `\"id\"` or `\"orcid\"` key\n",
    "    fields (str or list[str], optional):\n",
    "      One or more top‑level work attributes to include in each returned record.\n",
    "      Passed directly to the API’s `select` parameter. If None, all fields are returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing `requests.Session` to reuse for HTTP calls. If None, a temporary\n",
    "      session is created and closed automatically.\n",
    "\n",
    "  Returns:\n",
    "    list[dict]:\n",
    "      A list of OpenAlex work JSON objects authored by the specified author,\n",
    "      in no particular order beyond the API’s internal pagination sequence.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If the underlying HTTP request fails for any non‑rate‑limit error.\n",
    "  \"\"\"\n",
    "  params = _fields2params(fields)\n",
    "  url = f\"{BASE_URL}works\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_id = standardize_author_ids(author_id, session=_session)[0]\n",
    "    params['filter'] = f\"author.id:{author_id}\"\n",
    "    data = request_entities(url, params=params, session=_session)['results']\n",
    "  return data\n",
    "\n",
    "\n",
    "def fetch_citing_works_from_work(\n",
    "  work_id,\n",
    "  fields = None,\n",
    "  session: requests.Session = None\n",
    ") -> list:\n",
    "  \"\"\"\n",
    "  Retrieve all works that cite a specific OpenAlex work.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes the input `work_id` (which may be a DOI, OpenAlex ID, URL, or dict)\n",
    "       into the canonical OpenAlex work URL.\n",
    "    2. Queries the OpenAlex `/works` endpoint using the `cites:` filter to page through\n",
    "       every work that references the given work.\n",
    "    3. Collects and returns the full list of citing work JSON objects, optionally filtered\n",
    "       to include only the specified fields.\n",
    "\n",
    "  Args:\n",
    "    work_id (str or dict):\n",
    "      A work identifier, which may be:\n",
    "        - An OpenAlex work URL (e.g. \"https://openalex.org/W123456\")\n",
    "        - An OpenAlex work ID (e.g. \"W123456\")\n",
    "        - A DOI URL or code (e.g. \"https://doi.org/10.1000/xyz123\" or \"10.1000/xyz123\")\n",
    "        - A dict containing at least an `\"id\"` or `\"doi\"` key\n",
    "    fields (str or list[str], optional):\n",
    "      One or more top‑level work attributes to include in each returned record.\n",
    "      Passed directly to the API’s `select` parameter. If None, all fields are returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing `requests.Session` to reuse for HTTP calls. If None, a temporary\n",
    "      session is created and closed automatically.\n",
    "\n",
    "  Returns:\n",
    "    list[dict]:\n",
    "      A list of OpenAlex work JSON objects that cite the specified work,\n",
    "      in no particular order beyond the API’s internal pagination sequence.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any underlying HTTP request fails for a non‑rate‑limit error.\n",
    "  \"\"\"\n",
    "  params = _fields2params(fields)\n",
    "  params['per-page'] = 200\n",
    "  url = f\"{BASE_URL}works\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    work_id = standardize_work_ids(work_id, session=_session)[0]\n",
    "    params['filter'] = 'cites:' + work_id\n",
    "    data = request_entities(url, params=params, session=_session)['results']\n",
    "  return data\n",
    "\n",
    "\n",
    "# --- Compound/Relationship Fetchers ---\n",
    "def fetch_authors_from_work(\n",
    "  work_id,\n",
    "  fields = None,\n",
    "  session: requests.Session = None\n",
    ") -> list:\n",
    "  \"\"\"\n",
    "  Retrieve all authors credited on a specific OpenAlex work.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes the input `work_id` (which may be a DOI, OpenAlex ID, URL, or dict)\n",
    "       into the canonical OpenAlex work URL.\n",
    "    2. Fetches the work’s basic record (including its `authorships` array) via the\n",
    "       `/works` endpoint.\n",
    "    3. Extracts each author’s OpenAlex URL/ID from the `authorships` entries.\n",
    "    4. Batch‑fetches the full author records (optionally filtered to `fields`) via\n",
    "       the `/authors` endpoint.\n",
    "\n",
    "  Args:\n",
    "    work_id (str or dict):\n",
    "      A work identifier, which may be:\n",
    "        - An OpenAlex work URL (e.g. \"https://openalex.org/W123456\")\n",
    "        - An OpenAlex work ID (e.g. \"W123456\")\n",
    "        - A DOI URL or code (e.g. \"https://doi.org/10.1000/xyz123\" or \"10.1000/xyz123\")\n",
    "        - A dict containing at least an `\"id\"` or `\"doi\"` key\n",
    "    fields (str or list[str], optional):\n",
    "      One or more top‑level author attributes to include in each returned record.\n",
    "      Passed directly to the API’s `select` parameter. If None, all author fields are returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing `requests.Session` to reuse for HTTP calls. If None, a temporary\n",
    "      session is created and closed automatically.\n",
    "\n",
    "  Returns:\n",
    "    list[dict]:\n",
    "      A list of OpenAlex author JSON objects corresponding to the work’s authors,\n",
    "      in the same order as they appear in the work’s `authorships` list.\n",
    "      Returns an empty list (and issues a warning) if the work has no `authorships`.\n",
    "\n",
    "  Raises:\n",
    "    RuntimeError:\n",
    "      If any author lookup fails (e.g., an expected author ID cannot be retrieved).\n",
    "    HTTPError:\n",
    "      If any underlying HTTP request fails for a non‑rate‑limit error.\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    work_id = standardize_work_ids(work_id, session=_session)[0]\n",
    "    work = fetch_works(work_id, fields=['id', 'authorships'], session=_session)[0]\n",
    "    if not work.get('authorships'):\n",
    "      warnings.warn(f\"No authorships: work ID {work_id}\")\n",
    "      return []\n",
    "    author_ids = [auth.get('author').get('id') for auth in work.get('authorships')]\n",
    "    authors = fetch_authors(author_ids, fields=fields, session=_session)\n",
    "  return authors\n",
    "\n",
    "\n",
    "def fetch_authors_from_works(\n",
    "  work_ids,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve authors for one or more OpenAlex works, optionally keyed by work ID.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes each entry in `work_ids` to its canonical OpenAlex work URL.\n",
    "    2. For each work, fetches its authorships via `fetch_authors_from_work`.\n",
    "    3. Aggregates all author records into a single flat list, or, if\n",
    "       `keep_parent_ids=True`, returns a dict mapping each work’s URL to\n",
    "       its list of author JSON objects.\n",
    "\n",
    "  Args:\n",
    "    work_ids (str or list[str] or dict or list[dict]):\n",
    "      A single work identifier or a collection thereof. Each item may be:\n",
    "        - An OpenAlex work URL (e.g. \"https://openalex.org/W123456\")\n",
    "        - An OpenAlex work ID (e.g. \"W123456\")\n",
    "        - A DOI URL or code (e.g. \"https://doi.org/10.1000/xyz123\" or \"10.1000/xyz123\")\n",
    "        - A dict containing at least an `\"id\"` or `\"doi\"` key\n",
    "    fields (str or list[str], optional):\n",
    "      Top‑level author fields to include in each returned record. Passed through\n",
    "      to the API’s `select` parameter; if None, the full author record is returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing HTTP session to reuse for all requests. If None, a temporary\n",
    "      session is created and closed internally.\n",
    "    keep_parent_ids (bool):\n",
    "      - If False (default), returns a flat `list[dict]` of all authors across\n",
    "        the specified works.\n",
    "      - If True, returns a `dict` whose keys are each work’s OpenAlex URL and\n",
    "        whose values are the `list[dict]` of that work’s authors.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list[dict]]:\n",
    "      Depending on `keep_parent_ids`, either:\n",
    "        - A flat list of author JSON objects (duplicates possible if authors\n",
    "          appear on multiple works), or\n",
    "        - A mapping from each work’s URL to its list of author JSON objects.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any underlying API call fails with a non‑rate‑limit error.\n",
    "    RuntimeError:\n",
    "      If author lookup for any work fails to retrieve the expected data.\n",
    "  \"\"\"\n",
    "  authors = {} if keep_parent_ids else []\n",
    "  with _conditional_session(session) as _session:\n",
    "    work_ids = standardize_work_ids(work_ids, session=_session)\n",
    "    _fetcher = functools_partial(fetch_authors_from_work, fields=fields, session=_session)\n",
    "    for wid in tqdm(work_ids, desc=\"Fetching authors from works\"):\n",
    "      if keep_parent_ids:\n",
    "        authors[wid] = _fetcher(wid)\n",
    "      else:\n",
    "        authors.extend(_fetcher(wid))\n",
    "  return authors\n",
    "\n",
    "\n",
    "def fetch_coauthors_from_author(\n",
    "  author_id,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve coauthors for a given author, across all of their works.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes the input `author_id` to its canonical OpenAlex URL.\n",
    "    2. Fetches all works authored by that author.\n",
    "    3. For each work, retrieves its list of authors.\n",
    "    4. Aggregates these coauthor records into either:\n",
    "       - A flat list of coauthor JSON objects (if `keep_parent_ids=False`), or\n",
    "       - A dict mapping each work’s URL to its list of coauthor JSON objects\n",
    "         (if `keep_parent_ids=True`).\n",
    "\n",
    "  Args:\n",
    "    author_id (str or dict):\n",
    "      An ORCID, OpenAlex author URL/ID, or a dict containing an \"id\" or \"orcid\".\n",
    "    fields (str or list[str], optional):\n",
    "      Specific top-level author fields to include in each returned record.\n",
    "      Passed through to the API’s `select` parameter; if None, the full record\n",
    "      is returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing HTTP session to reuse for all requests. If None, a new session\n",
    "      is opened and closed internally.\n",
    "    keep_parent_ids (bool, default=False):\n",
    "      - If False: returns a flat list of all coauthor JSON dicts.\n",
    "      - If True: returns a dict where each key is a work’s OpenAlex URL and each\n",
    "        value is the list of coauthor JSON dicts for that work.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list[dict]]:\n",
    "      Depending on `keep_parent_ids`, either a flat list of coauthor records\n",
    "      (duplicates possible if the same coauthor appears on multiple works), or\n",
    "      a mapping from work URLs to their respective coauthor lists.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any underlying API request fails with a non‑rate‑limit error.\n",
    "    RuntimeError:\n",
    "      If a work or author lookup does not return the expected data.\n",
    "  \"\"\"\n",
    "  _looper = functools_partial(tqdm, desc=\"Fetching target author's coauthors\")\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_id = standardize_author_ids(author_id, session=_session)[0]\n",
    "    works = fetch_works_from_author(author_id, fields=['id'], session=_session)\n",
    "    work_ids = [wk['id'] for wk in works]\n",
    "    coauthors = fetch_authors_from_works(work_ids, fields=fields, session=_session, keep_parent_ids=keep_parent_ids)\n",
    "  return coauthors\n",
    "\n",
    "\n",
    "def fetch_citing_works_from_works(\n",
    "  work_ids,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve works that cite any of the given works.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes each input in `work_ids` to its canonical OpenAlex work URL.\n",
    "    2. For each work URL, fetches all works that cite it.\n",
    "    3. Aggregates the citing-work records into either:\n",
    "       - A flat list of work JSON objects (if `keep_parent_ids=False`), or\n",
    "       - A dict mapping each original work’s URL to its list of its citing works\n",
    "         (if `keep_parent_ids=True`).\n",
    "\n",
    "  Args:\n",
    "    work_ids (str or list[str] or dict or list[dict]):\n",
    "      One or more OpenAlex work URLs/IDs, DOIs, or work JSON dicts containing an 'id'.\n",
    "    fields (str or list[str], optional):\n",
    "      Specific top-level work fields to include in each returned record.\n",
    "      Passed through to the API’s `select` parameter; if None, the full record is returned.\n",
    "    session (requests.Session, optional):\n",
    "      An existing HTTP session to reuse for all requests. If None, a new session is\n",
    "      opened and closed internally.\n",
    "    keep_parent_ids (bool, default=False):\n",
    "      - If False: returns a flat list of all citing-work JSON dicts.\n",
    "      - If True: returns a dict where each key is an original work’s OpenAlex URL and\n",
    "        each value is the list of works that cite that work.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list[dict]]:\n",
    "      Depending on `keep_parent_ids`, either a flat list of citing-work records, or\n",
    "      a mapping from each input work URL to its respective list of citing works.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any underlying API request fails with an HTTP error other than rate limiting.\n",
    "    RuntimeError:\n",
    "      If a lookup for a work or its citing works does not return the expected data.\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    work_ids = standardize_work_ids(work_ids, session=_session)\n",
    "    citing_works = {} if keep_parent_ids else []\n",
    "    _fetcher = functools_partial(fetch_citing_works_from_work, fields=fields, session=_session)\n",
    "    for wid in tqdm(work_ids, desc=\"Fetching works that cite target work\"):\n",
    "      if keep_parent_ids:\n",
    "        citing_works[wid] = _fetcher(wid)\n",
    "      else:\n",
    "        citing_works.extend(_fetcher(wid))\n",
    "  return citing_works\n",
    "\n",
    "\n",
    "def fetch_citing_works_from_author(\n",
    "  author_id,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve all works that cite any publication by a given author.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes the `author_id` to its canonical OpenAlex URL.\n",
    "    2. Fetches all works authored by that author (only IDs).\n",
    "    3. For each of those works, fetches all works that cite it.\n",
    "    4. Aggregates results into either:\n",
    "       - A flat list of citing-work JSON dicts (if `keep_parent_ids=False`), or\n",
    "       - A dict mapping each original work’s URL to its list of its citing works\n",
    "         (if `keep_parent_ids=True`).\n",
    "\n",
    "  Args:\n",
    "    author_id (str or dict):\n",
    "      An ORCID, OpenAlex author URL/ID, or an author JSON dict containing an 'id'.\n",
    "    fields (str or list[str], optional):\n",
    "      Specific top-level fields of each citing work to include (passed to the API’s\n",
    "      `select` parameter). If None, the API returns the full work record.\n",
    "    session (requests.Session, optional):\n",
    "      If provided, reuses this HTTP session for all requests; otherwise a new session\n",
    "      is created and closed internally.\n",
    "    keep_parent_ids (bool, default=False):\n",
    "      - False: returns a flat list of all citing-work records.\n",
    "      - True: returns a dict mapping each authored work’s URL to its list of citing works.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list[dict]]:\n",
    "      Depending on `keep_parent_ids`, either:\n",
    "        - A flat list of works that cite any of the author’s works.\n",
    "        - A mapping from each authored work’s URL to its list of citing works.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any API request fails with a non-retryable HTTP error.\n",
    "    RuntimeError:\n",
    "      If expected data is missing (e.g., unable to retrieve works or citations).\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_id = standardize_author_ids(author_id, session=_session)[0]\n",
    "    works = fetch_works_from_author(author_id, fields=['id'], session=_session)\n",
    "    work_ids = [wk['id'] for wk in works]\n",
    "    citing_works = fetch_citing_works_from_works(work_ids, fields=fields, session=_session, keep_parent_ids=keep_parent_ids)\n",
    "  return citing_works\n",
    "\n",
    "\n",
    "def fetch_citing_works_from_authors(\n",
    "  author_ids,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve all works that cite any publications by one or more authors.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes each entry in `author_ids` to its canonical OpenAlex author URL.\n",
    "    2. For each author, fetches all works they have published.\n",
    "    3. For each of those works, fetches all works that cite it.\n",
    "    4. Aggregates results into either:\n",
    "       - A flat list of all citing-work records (if `keep_parent_ids=False`), or\n",
    "       - A dict mapping each author’s URL to the list of works that cite any of their works\n",
    "         (if `keep_parent_ids=True`).\n",
    "\n",
    "  Args:\n",
    "    author_ids (str or list[str] or dict or list[dict]):\n",
    "      An ORCID, OpenAlex author URL/ID, or author JSON dict (or a list thereof).\n",
    "    fields (str or list[str], optional):\n",
    "      Specific top-level fields of each citing work to include (passed to the API’s\n",
    "      `select` parameter). If None, the API returns the full work records.\n",
    "    session (requests.Session, optional):\n",
    "      If provided, reuses this HTTP session for all requests; otherwise a new session\n",
    "      is created and closed internally.\n",
    "    keep_parent_ids (bool, default=False):\n",
    "      - False: returns a flat list of all works citing any of the authors’ works.\n",
    "      - True: returns a dict mapping each author’s URL to its list of citing works.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list of dict]:\n",
    "      Depending on `keep_parent_ids`, either a flat list of citing-work records,\n",
    "      or a mapping from each author URL to its list of citing works.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any API request fails with a non-retryable HTTP error.\n",
    "    RuntimeError:\n",
    "      If expected data is missing or if no works/citations can be retrieved.\n",
    "  \"\"\"\n",
    "  _looper = functools_partial(tqdm, desc=\"Fetching authors' citing works\")\n",
    "  citing_works = {} if keep_parent_ids else []\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_ids = standardize_author_ids(author_ids, session=_session)\n",
    "    _fetcher = functools_partial(fetch_citing_works_from_author, fields=fields, session=_session, keep_parent_ids=False) # TODO: add multi-tiered parent IDs?\n",
    "    for aid in _looper(author_ids):\n",
    "      if keep_parent_ids:\n",
    "        citing_works[aid] = _fetcher(aid)\n",
    "      else:\n",
    "        citing_works.extend(_fetcher(aid))\n",
    "  return citing_works\n",
    "\n",
    "\n",
    "def fetch_citing_authors_from_work(\n",
    "  work_id,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve all authors whose works cite a specified publication.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes `work_id` to its canonical OpenAlex URL.\n",
    "    2. Fetches all works that cite the given work (only the work IDs by default).\n",
    "    3. For each citing work, extracts its list of authors.\n",
    "    4. Aggregates those authors into either:\n",
    "       - A flat list of author records (if `keep_parent_ids=False`), or\n",
    "       - A dict mapping the cited work’s URL to the list of its citing authors\n",
    "         (if `keep_parent_ids=True`).\n",
    "\n",
    "  Args:\n",
    "    work_id (str or dict):\n",
    "      An OpenAlex work URL/ID, DOI URL/ID, or partial work JSON dict containing `'id'`.\n",
    "    fields (str or list[str], optional):\n",
    "      Specific top-level fields of each author to include (passed to the API’s\n",
    "      `select` parameter). If None, the full author records are returned.\n",
    "    session (requests.Session, optional):\n",
    "      If provided, reuses this HTTP session for all requests; otherwise a new session\n",
    "      is created and closed internally.\n",
    "    keep_parent_ids (bool, default=False):\n",
    "      - False: returns a flat list of author records across all citing works.\n",
    "      - True: returns a dict with the single key being the cited work’s URL and the\n",
    "        value being its list of citing-author records.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list of dict]:\n",
    "      Depending on `keep_parent_ids`, either a flat list of author JSON dicts,\n",
    "      or a mapping `{ work_id: [author_dict, …] }`.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any underlying API request fails with a non-retryable HTTP error.\n",
    "    RuntimeError:\n",
    "      If expected data (e.g. authorships) is missing or cannot be retrieved.\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    work_id = standardize_work_ids(work_id, session=_session)[0]\n",
    "    citing_works = fetch_citing_works_from_work(work_id, fields=['id'], session=_session)\n",
    "    cwork_ids = [cwk['id'] for cwk in citing_works]\n",
    "    citing_authors = fetch_authors_from_works(cwork_ids, fields=fields, session=_session, keep_parent_ids=keep_parent_ids)\n",
    "  return citing_authors\n",
    "\n",
    "\n",
    "def fetch_citing_authors_from_works(\n",
    "  work_ids,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve all authors whose works cite any publication in a given list.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes each entry in `work_ids` to canonical OpenAlex URLs.\n",
    "    2. Fetches all works citing each target work.\n",
    "    3. Collects the author lists for those citing works.\n",
    "    4. Returns either:\n",
    "       - A flat list of author records (if `keep_parent_ids=False`), or\n",
    "       - A dict mapping each target work’s URL to the list of its citing authors\n",
    "         (if `keep_parent_ids=True`).\n",
    "\n",
    "  Args:\n",
    "    work_ids (str or list[str] or dict or list[dict]):\n",
    "      A single or list of OpenAlex work URLs/IDs, DOI URLs/IDs, or partial\n",
    "      work JSON dicts containing `'id'`.\n",
    "    fields (str or list[str], optional):\n",
    "      Field or list of fields to include for each citing author (passed to the\n",
    "      API `select` parameter). If None, full author records are returned.\n",
    "    session (requests.Session, optional):\n",
    "      If provided, uses this HTTP session for all requests; otherwise creates\n",
    "      and closes a new session internally.\n",
    "    keep_parent_ids (bool, default=False):\n",
    "      - False: returns a flat list of author JSON dicts across all citing works.\n",
    "      - True: returns a dict `{ work_id: [author_dict, …] }` for each input work.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list[dict]]:\n",
    "      Depending on `keep_parent_ids`, either a flat list of author records,\n",
    "      or a mapping from each original work ID to its list of citing-author records.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any API request fails irrecoverably.\n",
    "    RuntimeError:\n",
    "      If expected data is missing or cannot be retrieved.\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    work_ids = standardize_work_ids(work_ids, session=_session)\n",
    "    citing_works = fetch_citing_works_from_works(work_ids, fields=['id'], session=_session, keep_parent_ids=False) # TODO: enable two-tiered parent ID–saving?\n",
    "    citing_work_ids = [cwk['id'] for cwk in citing_works]\n",
    "    citing_authors = fetch_authors_from_works(citing_work_ids, fields=fields, session=_session, keep_parent_ids=keep_parent_ids)\n",
    "  return citing_authors\n",
    "\n",
    "\n",
    "def fetch_citing_authors_from_author(\n",
    "  author_id,\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_parent_ids: bool = False\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve all authors who have cited any publication by a given author.\n",
    "\n",
    "  This function:\n",
    "    1. Normalizes the input `author_id` to a canonical OpenAlex author URL.\n",
    "    2. Fetches all works authored by that author.\n",
    "    3. Retrieves all works that cite each of those works.\n",
    "    4. Gathers the author lists for those citing works.\n",
    "    5. Returns either:\n",
    "       - A flat list of citing-author records (if `keep_parent_ids=False`), or\n",
    "       - A mapping from each citing-work URL back to its list of citing-author records\n",
    "         (if `keep_parent_ids=True`).\n",
    "\n",
    "  Args:\n",
    "    author_id (str or dict):\n",
    "      ORCID, OpenAlex author URL/ID, or an author JSON dict containing an `'id'` key.\n",
    "    fields (str or list[str], optional):\n",
    "      Field or list of fields to include for each citing author (passed to the API `select` parameter).\n",
    "      If None, full author records are returned.\n",
    "    session (requests.Session, optional):\n",
    "      If provided, uses this HTTP session for all requests; otherwise creates\n",
    "      and closes a new session internally.\n",
    "    keep_parent_ids (bool, default=False):\n",
    "      - False: returns a flat list of author JSON dicts across all citing works.\n",
    "      - True: returns a dict `{ work_id: [author_dict, …] }` mapping each citing-work URL\n",
    "        to its list of author records.\n",
    "\n",
    "  Returns:\n",
    "    list[dict] or dict[str, list[dict]]:\n",
    "      Depending on `keep_parent_ids`, either a flat list of author records,\n",
    "      or a mapping from each citing-work ID to its list of citing-author records.\n",
    "\n",
    "  Raises:\n",
    "    HTTPError:\n",
    "      If any API request fails after retries.\n",
    "    RuntimeError:\n",
    "      If any expected data cannot be retrieved or is missing.\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_id = standardize_author_ids(author_id, session=_session)[0]\n",
    "    works = fetch_works_from_author(author_id, fields=['id'], session=_session, keep_parent_ids=False) # TODO: add multi-tiered parent IDs?\n",
    "    work_ids = [wk['id'] for wk in works]\n",
    "    citing_works = fetch_citing_works_from_works(work_ids, fields=['id'], session=_session, keep_parent_ids=False) # TODO: add multi-tiered parent IDs?\n",
    "    citing_work_ids = [cwk['id'] for cwk in citing_works]\n",
    "    citing_authors = fetch_authors_from_works(citing_work_ids, fields=fields, session=_session, keep_parent_ids=keep_parent_ids)\n",
    "  return citing_authors\n",
    "\n",
    "\n",
    "# --- Conversion Utilities ---\n",
    "def _to_set(\n",
    "  entities: list\n",
    ") -> set:\n",
    "  \"\"\"\n",
    "  Build a set of unique display names from a list of OpenAlex entity records.\n",
    "\n",
    "  Iterates through each entity dict in the input list, extracts the value\n",
    "  associated with the 'display_name' key, and returns a set of these values,\n",
    "  thereby deduplicating any repeated names.\n",
    "\n",
    "  Args:\n",
    "    entities (list of dict): A list where each element is an OpenAlex entity\n",
    "      JSON dict containing at least the 'display_name' key mapping to a string.\n",
    "\n",
    "  Returns:\n",
    "    set of str: The unique display names found in the input entities.\n",
    "\n",
    "  Raises:\n",
    "    KeyError: If any entity in the list does not include the 'display_name' key.\n",
    "  \"\"\"\n",
    "  return set(e['display_name'] for e in entities)\n",
    "\n",
    "\n",
    "def _to_dict(\n",
    "  entities: list\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Build a mapping from each entity’s unique ID to its display name.\n",
    "\n",
    "  Processes a list of OpenAlex entity records, each of which must include\n",
    "  the keys 'id' (a unique URI string) and 'display_name' (the human-readable\n",
    "  name). Returns a dictionary where each key is an entity ID and each value\n",
    "  is the corresponding display name.\n",
    "\n",
    "  Args:\n",
    "    entities (list of dict): A list of entity JSON dicts, each containing:\n",
    "      - 'id' (str): The unique identifier (e.g. \"https://openalex.org/A12345\").\n",
    "      - 'display_name' (str): The name to display for that entity.\n",
    "\n",
    "  Returns:\n",
    "    dict[str, str]: A dictionary mapping entity IDs to their display names.\n",
    "\n",
    "  Raises:\n",
    "    KeyError: If any entity dict is missing the 'id' or 'display_name' key.\n",
    "  \"\"\"\n",
    "  return {e['id']: e['display_name'] for e in entities}\n",
    "\n",
    "\n",
    "def _to_dataframe(\n",
    "  entities: list,\n",
    "  df_cols = None,\n",
    "  override_defaults: bool = False\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Construct a pandas DataFrame from a sequence of OpenAlex entity records.\n",
    "\n",
    "  This utility converts each entity (a JSON-like dict) into a row in a DataFrame.\n",
    "  By default, every row will include the columns:\n",
    "    - 'id'             : the entity’s unique identifier (e.g. URL)\n",
    "    - 'display_name'   : the human-readable name\n",
    "    - 'occurrences'    : a numeric count attached to the entity\n",
    "\n",
    "  Additional fields may be included or renamed via `df_cols`:\n",
    "    - If `df_cols` is a string, that single field is added as a new column.\n",
    "    - If `df_cols` is a list, each named field is added as its own column.\n",
    "    - If `df_cols` is a dict, its keys are JSON fields to extract and its\n",
    "      values are the desired column names in the DataFrame.\n",
    "\n",
    "  If `override_defaults` is True, the default columns ('id', 'display_name',\n",
    "  'occurrences') are omitted, and **only** the fields specified in `df_cols`\n",
    "  will appear.\n",
    "\n",
    "  Args:\n",
    "    entities (list of dict): Each dict represents an OpenAlex entity and may\n",
    "      contain arbitrary keys. At minimum, default mode expects 'id',\n",
    "      'display_name', and 'occurrences'.\n",
    "    df_cols (str, list, or dict, optional): Specifies extra fields to include:\n",
    "      - str: single field name to pull in\n",
    "      - list of str: multiple field names\n",
    "      - dict: mapping from JSON key → desired column name\n",
    "      If None, no extra fields beyond the defaults are added.\n",
    "    override_defaults (bool): If True, drop the default columns and include\n",
    "      only those specified in `df_cols`. In override mode, `df_cols` **must**\n",
    "      be non-empty.\n",
    "\n",
    "  Returns:\n",
    "    pandas.DataFrame: A DataFrame whose columns consist of the default columns\n",
    "      (unless overridden) plus any additional columns requested via `df_cols`.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If `override_defaults` is True but `df_cols` is None or empty.\n",
    "  \"\"\"\n",
    "  rows = []\n",
    "  for e in entities:\n",
    "    if override_defaults:\n",
    "      if (not df_cols) or (len(df_cols) == 0):\n",
    "        raise ValueError(\"`override_defaults` is True, so `df_cols` must include at least one attribute.\")\n",
    "      row = {}\n",
    "    else:\n",
    "      row = {'id': e.get('id'), 'display_name': e.get('display_name'), 'occurrences': e.get('occurrences')}\n",
    "    if df_cols:\n",
    "      if isinstance(df_cols, str):\n",
    "        row[df_cols] = e.get(df_cols)\n",
    "      elif hasattr(df_cols, '__iter__') and not isinstance(df_cols, dict):\n",
    "        for f in df_cols:\n",
    "          row[f] = e.get(f)\n",
    "      elif isinstance(df_cols, dict):\n",
    "        for k, v in df_cols.items():\n",
    "          row[v] = e.get(k)\n",
    "    rows.append(row)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# --- Primary Functions ---\n",
    "def _format_output(\n",
    "  entities,\n",
    "  output: str,\n",
    "  **kwargs\n",
    "):\n",
    "# def _format_output(\n",
    "#     entities: list[dict],\n",
    "#     output: str,\n",
    "#     **kwargs\n",
    "# ) -> Union[set, dict, \"pandas.DataFrame\"]:\n",
    "  \"\"\"\n",
    "  Convert a list of entity records into the specified output format.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  entities : list of dict\n",
    "    A list of JSON-like entity records to format.\n",
    "  output : str\n",
    "    Specifies the desired output type. Must be one of:\n",
    "    - 'set': return a set of entity identifiers (via `_to_set`)\n",
    "    - 'dict': return a dict mapping entity identifiers to records (via `_to_dict`)\n",
    "    - 'df' : return a pandas DataFrame (via `_to_dataframe`)\n",
    "  **kwargs : optional\n",
    "    Additional keyword arguments forwarded to `_to_dataframe()` when `output='df'`. Supported keys:\n",
    "    - df_cols (str, list of str, or dict): fields to include or mapping of JSON keys to column names.\n",
    "      Required if `output='df'`.\n",
    "    - override_defaults (bool): if True, omit the default columns (`id`, `display_name`, `occurrences`)\n",
    "      and only include those specified in `df_cols`.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  set or dict or pandas.DataFrame\n",
    "    - set: a set of entity IDs\n",
    "    - dict: a mapping of entity IDs to their record dicts\n",
    "    - pandas.DataFrame: a DataFrame containing the requested columns\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  ValueError\n",
    "    If `output` is not one of 'set', 'dict', or 'df'.\n",
    "  \"\"\"\n",
    "  if output == 'set':\n",
    "    return _to_set(entities)\n",
    "  elif output == 'dict':\n",
    "    return _to_dict(entities)\n",
    "  elif output == 'df':\n",
    "    return _to_dataframe(entities, **kwargs)\n",
    "  else:\n",
    "    raise ValueError(f\"Unsupported output type: {output}\")\n",
    "\n",
    "\n",
    "def get_coauthors(\n",
    "  author_id,\n",
    "  output: str = 'set', \n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_works: bool = False,\n",
    "  work_fields = None,\n",
    "  **kwargs\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve and format the coauthor network for a given author.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  author_id : str or dict\n",
    "    The target author identifier. Can be:\n",
    "    - ORCID string (e.g., \"0000-0001-2345-6789\")\n",
    "    - OpenAlex URL or ID (e.g., \"https://openalex.org/A1234567890\" or \"A1234567890\")\n",
    "    - A JSON-like dict representing an OpenAlex author record.\n",
    "  output : {'set', 'dict', 'df'}, default 'set'\n",
    "    The desired return format:\n",
    "    - 'set': return a set of coauthor IDs.\n",
    "    - 'dict': return a dict mapping coauthor IDs to author record dicts,\n",
    "      each augmented with:\n",
    "        • 'linking_works' (list of work IDs or work dicts)\n",
    "        • 'occurrences' (int count of shared works)\n",
    "    - 'df' : return a pandas.DataFrame with one row per coauthor,\n",
    "      including columns for requested fields plus 'linking_works' and 'occurrences'.\n",
    "  fields : str or list of str, optional\n",
    "    Fields to fetch for each coauthor record from the API. If None, defaults\n",
    "    to all available fields.\n",
    "  session : requests.Session, optional\n",
    "    An existing `requests.Session` to reuse for HTTP requests. If None,\n",
    "    a temporary session will be created and closed automatically.\n",
    "  keep_works : bool, default False\n",
    "    Whether to include the list of linking works for each coauthor under\n",
    "    the key 'linking_works'. If False, only counts are recorded.\n",
    "  work_fields : str or list of str, optional\n",
    "    Fields to fetch when retrieving works to build the coauthorship links.\n",
    "    Used only if `keep_works=True`. If None, only work IDs are fetched.\n",
    "  **kwargs : dict, optional\n",
    "    Additional keyword arguments forwarded to `_format_output()` when\n",
    "    `output='df'`. Supported keys include:\n",
    "    - `df_cols`: fields or mapping of JSON keys to DataFrame columns (required\n",
    "      if `output='df'`)\n",
    "    - `override_defaults`: bool flag to override default columns in DataFrame.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  set or dict or pandas.DataFrame\n",
    "    - set: a set of coauthor IDs.\n",
    "    - dict: mapping from coauthor ID to author record dict with\n",
    "      'linking_works' and 'occurrences'.\n",
    "    - pandas.DataFrame: rows of coauthor information, including\n",
    "      one column per requested field plus 'linking_works' and 'occurrences'.\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  ValueError\n",
    "    If `output` is not one of 'set', 'dict', or 'df', or if required\n",
    "    DataFrame parameters (e.g., `df_cols`) are missing when `output='df'`.\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_id = standardize_author_ids(author_id, session=_session)\n",
    "    _work_fields = work_fields if (keep_works and work_fields) else ['id']\n",
    "    works = fetch_works_from_author(author_id, fields=_work_fields, session=_session)\n",
    "    works = {wk['id']: wk for wk in works}\n",
    "    work_ids = list(works.keys())\n",
    "    coauthors_per_work = fetch_authors_from_works(work_ids, fields=fields, session=_session, keep_parent_ids=True)\n",
    "  coauthors = {}\n",
    "  for wid in tqdm(work_ids, desc=\"Building coauthor-work link network\"):\n",
    "    wk = works[wid] if keep_works else wid\n",
    "    for cauth in coauthors_per_work[wid]:\n",
    "      caid = cauth['id']\n",
    "      if coauthors.get(caid):\n",
    "        coauthors[caid]['linking_works'].append(wk)\n",
    "        coauthors[caid]['occurrences'] += 1\n",
    "      else:\n",
    "        coauthors[caid] = cauth\n",
    "        coauthors[caid]['linking_works'] = [wk]\n",
    "        coauthors[caid]['occurrences'] = 1\n",
    "  return _format_output(list(coauthors.values()), output, **kwargs)\n",
    "\n",
    "\n",
    "def get_citing_authors(\n",
    "  author_id,\n",
    "  output: str = 'set',\n",
    "  fields = None,\n",
    "  session: requests.Session = None,\n",
    "  keep_works: bool = False,\n",
    "  work_fields = None,\n",
    "  **kwargs\n",
    "):\n",
    "  \"\"\"\n",
    "  Retrieve and format the authors who cite a given target author.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  author_id : str or dict\n",
    "    Identifier for the target author. Can be:\n",
    "    - ORCID string (e.g., \"0000-0001-2345-6789\")\n",
    "    - OpenAlex URL or ID (e.g., \"https://openalex.org/A1234567890\" or \"A1234567890\")\n",
    "    - A JSON-like dict representing an OpenAlex author record.\n",
    "  output : {'set', 'dict', 'df'}, default 'set'\n",
    "    Desired output format:\n",
    "    - 'set': return a set of citing-author IDs.\n",
    "    - 'dict': return a dict mapping each citing-author ID to its record dict, \n",
    "      augmented with:\n",
    "        • 'linking_works': list of work IDs or work dicts that cite the target author\n",
    "        • 'occurrences': number of citing works\n",
    "    - 'df' : return a pandas.DataFrame with one row per citing author,\n",
    "      including requested fields plus 'linking_works' and 'occurrences'.\n",
    "  fields : str or list of str, optional\n",
    "    Fields to fetch for each citing-author record. If None, defaults to all available fields.\n",
    "  session : requests.Session, optional\n",
    "    An existing requests.Session for connection reuse. If None, a temporary session is created.\n",
    "  keep_works : bool, default False\n",
    "    If True, include the list of linking works for each citing author under 'linking_works'.\n",
    "    If False, only counts are recorded.\n",
    "  work_fields : str or list of str, optional\n",
    "    Fields to fetch when retrieving citing works. Used only if `keep_works=True`. \n",
    "    If None, only work IDs are fetched.\n",
    "  **kwargs : dict, optional\n",
    "    Additional keyword arguments forwarded to `_format_output()` when `output='df'`. Supported keys:\n",
    "    - `df_cols`: fields or mapping of JSON keys to DataFrame columns (required if `output='df'`)\n",
    "    - `override_defaults`: bool flag to override default columns in the DataFrame.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  set or dict or pandas.DataFrame\n",
    "    - set: a set of IDs of citing authors.\n",
    "    - dict: mapping from citing-author ID to its record dict with\n",
    "      'linking_works' and 'occurrences'.\n",
    "    - pandas.DataFrame: rows of citing-author data, including requested fields,\n",
    "      'linking_works', and 'occurrences'.\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  ValueError\n",
    "    If `output` is not one of 'set', 'dict', or 'df', or if required DataFrame\n",
    "    parameters (e.g., `df_cols`) are missing when `output='df'`.\n",
    "  \"\"\"\n",
    "  with _conditional_session(session) as _session:\n",
    "    author_id = standardize_author_ids(author_id, session=_session)\n",
    "    _work_fields = work_fields if (keep_works and work_fields) else ['id']\n",
    "    citing_works = fetch_citing_works_from_author(author_id, fields=_work_fields, session=_session)\n",
    "    citing_works = {cwk['id']: cwk for cwk in citing_works}\n",
    "    citing_work_ids = list(citing_works.keys())\n",
    "    citing_authors_per_work = fetch_authors_from_works(citing_work_ids, fields=fields, session=_session, keep_parent_ids=True)\n",
    "  citing_authors = {}\n",
    "  for cwid in tqdm(citing_work_ids, desc=\"Building citing author–work link network\"):\n",
    "    cwk = citing_works[cwid] if keep_works else cwid\n",
    "    for cauth in citing_authors_per_work[cwid]:\n",
    "      caid = cauth['id']\n",
    "      if citing_authors.get(caid):\n",
    "        citing_authors[caid]['linking_works'].append(cwk)\n",
    "        citing_authors[caid]['occurrences'] += 1\n",
    "      else:\n",
    "        citing_authors[caid] = cauth\n",
    "        citing_authors[caid]['linking_works'] = [cwk]\n",
    "        citing_authors[caid]['occurrences'] = 1\n",
    "  return _format_output(list(citing_authors.values()), output, **kwargs)\n",
    "\n",
    "\n",
    "def remove_author_list(\n",
    "  authors,\n",
    "  authors_subtract\n",
    "):\n",
    "  \"\"\"\n",
    "  Subtract one set of authors from another based on their IDs.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  authors : pandas.DataFrame\n",
    "    DataFrame of author records to filter. Must include an 'id' column.\n",
    "  authors_subtract : pandas.DataFrame\n",
    "    DataFrame of author records to remove. Must include an 'id' column.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pandas.DataFrame\n",
    "    A new DataFrame containing only those rows from `authors` whose 'id'\n",
    "    values are not present in `authors_subtract['id']`. The original index\n",
    "    and column order of `authors` are preserved.\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  NotImplementedError\n",
    "    If either `authors` or `authors_subtract` is not a pandas.DataFrame.\n",
    "\n",
    "  Examples\n",
    "  --------\n",
    "  >>> import pandas as pd\n",
    "  >>> authors = pd.DataFrame([{'id': 'A1', 'name': 'Alice'},\n",
    "  ...                         {'id': 'B2', 'name': 'Bob'}])\n",
    "  >>> to_remove = pd.DataFrame([{'id': 'B2', 'name': 'Bob'}])\n",
    "  >>> remove_author_list(authors, to_remove)\n",
    "     id   name\n",
    "  0  A1  Alice\n",
    "  \"\"\"\n",
    "  if all([isinstance(x, pd.DataFrame) for x in [authors, authors_subtract]]):\n",
    "    return authors.loc[~authors[\"id\"].isin(authors_subtract[\"id\"])]\n",
    "  else:\n",
    "    raise NotImplementedError(\"Currently the only implemented object type is pd.DataFrame.\")\n",
    "\n",
    "\n",
    "def extract_dict_key_to_column(\n",
    "  df: pd.DataFrame,\n",
    "  dict_col: str,\n",
    "  key: str,\n",
    "  new_col: str = None,\n",
    "  drop_origin: bool = False\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Extract a value for a given key from dictionaries in one DataFrame column\n",
    "  into its own new column.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df : pandas.DataFrame\n",
    "    Input DataFrame containing a column of dicts.\n",
    "  dict_col : str\n",
    "    Name of the column whose entries are dictionaries (or possibly other types).\n",
    "  key : str\n",
    "    The key to extract from each dictionary in `dict_col`.\n",
    "  new_col : str, optional\n",
    "    Name for the new column to hold extracted values.\n",
    "    If None (default), the new column will be named the same as `key`.\n",
    "  drop_origin : bool, default False\n",
    "    If True, drop the original `dict_col` column after extraction.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  pandas.DataFrame\n",
    "    A DataFrame with the new column (`new_col`) containing the values\n",
    "    extracted from each dict. Entries where the dict is missing,\n",
    "    not a dict, or does not contain `key` will be NaN/None.\n",
    "    If `drop_origin` is True, the original `dict_col` is removed.\n",
    "\n",
    "  Examples\n",
    "  --------\n",
    "  >>> import pandas as pd\n",
    "  >>> df = pd.DataFrame({'info': [{'x': 10}, {'x': 20}, None, {'y': 5}]})\n",
    "  >>> extract_dict_key_to_column(df, 'info', 'x', new_col='value')\n",
    "       info  value\n",
    "  0  {'x': 10}   10\n",
    "  1  {'x': 20}   20\n",
    "  2     None   None\n",
    "  3  {'y': 5}   None\n",
    "  \"\"\"\n",
    "  if new_col is None:\n",
    "    new_col = key\n",
    "  # Use .apply so that missing or non‐dict entries safely give NaN\n",
    "  df[new_col] = df[dict_col].apply(lambda d: d.get(key) if isinstance(d, dict) else None)\n",
    "  if drop_origin:\n",
    "    return df.drop(columns=[dict_col])\n",
    "  return df\n",
    "\n",
    "\n",
    "def list_by_sorted_hindex(\n",
    "  df: pd.DataFrame,\n",
    "  ttl: str = \"Authors sorted by h-index\",\n",
    "  max_count: int = 25,\n",
    "  min_occurrences: int = 1\n",
    ") -> None:\n",
    "  \"\"\"\n",
    "  Print a Markdown table of authors ranked by their h-index.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df : pandas.DataFrame\n",
    "    DataFrame containing at least the following columns:\n",
    "    - 'display_name' (str): the author’s name\n",
    "    - 'h_index' (int): the author’s h-index\n",
    "    - 'occurrences' (int): number of works linking to the target author\n",
    "    The DataFrame must be sorted in descending order by 'h_index' and\n",
    "    should have a monotonically increasing index (used as the rank).\n",
    "  ttl : str, default \"Authors sorted by h-index\"\n",
    "    Markdown header title printed above the table.\n",
    "  max_count : int, default 25\n",
    "    Maximum number of authors (rows) to display.\n",
    "  min_occurrences : int, default 1\n",
    "    Minimum value of 'occurrences' required for an author to be included.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  None\n",
    "    Prints the table via IPython.display.Markdown and returns None.\n",
    "\n",
    "  Examples\n",
    "  --------\n",
    "  >>> # Assume df is sorted by h_index descending and has 'display_name', 'h_index', 'occurrences'\n",
    "  >>> list_by_sorted_hindex(df, ttl=\"Top Coauthors\", max_count=10, min_occurrences=2)\n",
    "  \"\"\"\n",
    "  # Initialize\n",
    "  df_height = len(df)\n",
    "  max_count = min(df_height, max_count) if max_count else df_height\n",
    "  if max_count < 1:\n",
    "    raise ValueError(f\"`max_count` ({max_count}) cannot be < 0.\")\n",
    "  min_occurrences = min_occurrences if min_occurrences else 1\n",
    "  if min_occurrences < 1:\n",
    "    raise ValueError(f\"`min_occurrences` ({min_occurrences}) cannot be < 0.\")\n",
    "  enforce_min = lambda c: max(len(c), 3)\n",
    "  md_str = f\"### {ttl}\\n\\n\"\n",
    "  # Parse dataframe from formatting info\n",
    "  max_rank = len(df)\n",
    "  col1 = f\"{'Rank':{len(str(max_rank))}}\"\n",
    "  col1_w = enforce_min(col1)\n",
    "  col1_sep = '-' * col1_w\n",
    "  max_name_length = df['display_name'].str.len().max()\n",
    "  col2 = f\"{'Author':{max_name_length}}\"\n",
    "  col2_w = enforce_min(col2)\n",
    "  col2_sep = '-' * col2_w\n",
    "  max_hindex = df['h_index'].max()\n",
    "  col3 = f\"{'h-index':{len(str(max_hindex))}}\"\n",
    "  col3_w = enforce_min(col3)\n",
    "  col3_sep = '-' * col3_w\n",
    "  max_occurrences = df['occurrences'].max()\n",
    "  col4 = f\"{'occurrences':{len(str(max_occurrences))}}\"\n",
    "  col4_w = enforce_min(col4)\n",
    "  col4_sep = '-' * col4_w\n",
    "  # Define Markdown table\n",
    "  md_str += f\"| {col1:{col1_w}} | {col2:{col2_w}} | {col3:{col3_w}} | {col4:{col4_w}} |\\n\"\n",
    "  md_str += f\"| {col1_sep:{col1_w}} | {col2_sep:{col2_w}} | {col3_sep:{col3_w}} | {col4_sep:{col4_w}} |\\n\"\n",
    "  count = 1\n",
    "  for idx, auth in df.iterrows():\n",
    "    if count > max_count:\n",
    "      break\n",
    "    if auth['occurrences'] < min_occurrences:\n",
    "      continue\n",
    "    md_str += f\"| {idx:{col1_w}} | {auth['display_name']:{col2_w}} | {auth['h_index']:{col3_w}} | {auth['occurrences']:{col4_w}} |\\n\"\n",
    "    count += 1\n",
    "  md_str = md_str[:-1] # remove final '\\n'\n",
    "  display(Markdown(md_str))\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e791d5f2-233b-4d44-8b3a-4bd24e869ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orcid_MJ = \"0000-0002-3318-5801\"\n",
    "orcid_AR = \"0000-0002-8598-0815\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837367b-7082-4be0-8318-40582ebd6379",
   "metadata": {},
   "source": [
    "### Retrieve citing authors and coauthors (run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5844ef7a-e695-4ce4-840c-f39fe9d3d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['id', 'orcid', 'display_name', 'summary_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2973c7c8-d1bd-4b86-b165-3f1b084564e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching authors from works: 100%|████| 3/3 [00:02<00:00,  1.05it/s]\n",
      "Building coauthor-work link network: 100%|█| 3/3 [00:00<00:00, 24818\n",
      "Fetching works that cite target work: 100%|█| 3/3 [00:00<00:00,  3.1\n",
      "Fetching authors from works: 100%|██| 40/40 [00:29<00:00,  1.36it/s]\n",
      "Building citing author–work link network: 100%|█| 40/40 [00:00<00:00\n"
     ]
    }
   ],
   "source": [
    "target_orcid = orcid_MJ\n",
    "# Fetch coauthor and citing author dataframes\n",
    "coauthors_MJ = get_coauthors(target_orcid, fields=fields, output='df', df_cols=fields)\n",
    "citers_MJ = get_citing_authors(target_orcid, fields=fields, output='df', df_cols=fields)\n",
    "# Create dataframe of citing non-coauthors\n",
    "noncoauthors_MJ = remove_author_list(citers_MJ, coauthors_MJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb195e1-4d7c-47ab-8949-143c7fc2d0bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>occurrences</th>\n",
       "      <th>orcid</th>\n",
       "      <th>summary_stats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/A5040982088</td>\n",
       "      <td>Matt J. Jones</td>\n",
       "      <td>3</td>\n",
       "      <td>https://orcid.org/0000-0002-3318-5801</td>\n",
       "      <td>{'2yr_mean_citedness': 9.0, 'h_index': 1, 'i10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/A5079331777</td>\n",
       "      <td>A. J. Evans</td>\n",
       "      <td>2</td>\n",
       "      <td>https://orcid.org/0000-0002-3644-1313</td>\n",
       "      <td>{'2yr_mean_citedness': 5.375, 'h_index': 16, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/A5017739944</td>\n",
       "      <td>Brandon Johnson</td>\n",
       "      <td>2</td>\n",
       "      <td>https://orcid.org/0000-0002-4267-093X</td>\n",
       "      <td>{'2yr_mean_citedness': 2.8, 'h_index': 30, 'i1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/A5110959192</td>\n",
       "      <td>Matthew B. Weller</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{'2yr_mean_citedness': 6.666666666666667, 'h_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/A5034666199</td>\n",
       "      <td>J. C. Andrews‐Hanna</td>\n",
       "      <td>2</td>\n",
       "      <td>https://orcid.org/0000-0001-9374-7776</td>\n",
       "      <td>{'2yr_mean_citedness': 4.55, 'h_index': 39, 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://openalex.org/A5069915738</td>\n",
       "      <td>Sonia M. Tikoo-Schantz</td>\n",
       "      <td>1</td>\n",
       "      <td>https://orcid.org/0000-0001-9524-8284</td>\n",
       "      <td>{'2yr_mean_citedness': 0.31666666666666665, 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://openalex.org/A5043949808</td>\n",
       "      <td>J. T. Keane</td>\n",
       "      <td>1</td>\n",
       "      <td>https://orcid.org/0000-0002-4803-5793</td>\n",
       "      <td>{'2yr_mean_citedness': 3.0, 'h_index': 24, 'i1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://openalex.org/A5043039042</td>\n",
       "      <td>Fiona Nichols‐Fleming</td>\n",
       "      <td>1</td>\n",
       "      <td>https://orcid.org/0000-0002-7700-5139</td>\n",
       "      <td>{'2yr_mean_citedness': 1.0, 'h_index': 4, 'i10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id            display_name  occurrences  \\\n",
       "0  https://openalex.org/A5040982088           Matt J. Jones            3   \n",
       "1  https://openalex.org/A5079331777             A. J. Evans            2   \n",
       "2  https://openalex.org/A5017739944         Brandon Johnson            2   \n",
       "3  https://openalex.org/A5110959192       Matthew B. Weller            1   \n",
       "4  https://openalex.org/A5034666199     J. C. Andrews‐Hanna            2   \n",
       "5  https://openalex.org/A5069915738  Sonia M. Tikoo-Schantz            1   \n",
       "6  https://openalex.org/A5043949808             J. T. Keane            1   \n",
       "7  https://openalex.org/A5043039042   Fiona Nichols‐Fleming            1   \n",
       "\n",
       "                                   orcid  \\\n",
       "0  https://orcid.org/0000-0002-3318-5801   \n",
       "1  https://orcid.org/0000-0002-3644-1313   \n",
       "2  https://orcid.org/0000-0002-4267-093X   \n",
       "3                                   None   \n",
       "4  https://orcid.org/0000-0001-9374-7776   \n",
       "5  https://orcid.org/0000-0001-9524-8284   \n",
       "6  https://orcid.org/0000-0002-4803-5793   \n",
       "7  https://orcid.org/0000-0002-7700-5139   \n",
       "\n",
       "                                       summary_stats  \n",
       "0  {'2yr_mean_citedness': 9.0, 'h_index': 1, 'i10...  \n",
       "1  {'2yr_mean_citedness': 5.375, 'h_index': 16, '...  \n",
       "2  {'2yr_mean_citedness': 2.8, 'h_index': 30, 'i1...  \n",
       "3  {'2yr_mean_citedness': 6.666666666666667, 'h_i...  \n",
       "4  {'2yr_mean_citedness': 4.55, 'h_index': 39, 'i...  \n",
       "5  {'2yr_mean_citedness': 0.31666666666666665, 'h...  \n",
       "6  {'2yr_mean_citedness': 3.0, 'h_index': 24, 'i1...  \n",
       "7  {'2yr_mean_citedness': 1.0, 'h_index': 4, 'i10...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coauthors_MJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d20308-5f7e-49f7-a99d-2d1fb8c11c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Authors sorted by h-index\n",
       "\n",
       "| Rank | Author                 | h-index | occurrences |\n",
       "| ---- | ---------------------- | ------- | ----------- |\n",
       "|    0 | J. W. Head             |     129 |           9 |\n",
       "|   15 | Lionel Wilson          |      63 |           4 |\n",
       "|   24 | Long Xiao              |      46 |           4 |\n",
       "|   35 | Chunlai Li             |      37 |           6 |\n",
       "|   45 | Zhiyong Xiao           |      33 |           3 |\n",
       "|   56 | Jianzhong Liu          |      30 |           5 |\n",
       "|   82 | Qin Zhou               |      22 |           5 |\n",
       "|   86 | Le Qiao                |      21 |           3 |\n",
       "|   89 | Yuqi Qian              |      20 |           6 |\n",
       "|  116 | Xing Wang              |      15 |           3 |\n",
       "|  138 | T. C. Prissel          |      11 |           3 |\n",
       "|  143 | Adrien Broquet         |      10 |           3 |\n",
       "|  168 | Fanglu Luo             |       5 |           3 |\n",
       "|  169 | Yuan Chen              |       5 |           4 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noncoauthors = copy.deepcopy(noncoauthors_MJ)\n",
    "noncoauthors = extract_dict_key_to_column(noncoauthors, dict_col='summary_stats', key='h_index', new_col='h_index', drop_origin=False)\n",
    "\n",
    "# Sort by h-index descending\n",
    "noncoauthors_sorted = noncoauthors.sort_values(by='h_index', ascending=False, ignore_index=True)\n",
    "list_by_sorted_hindex(noncoauthors_sorted, max_count=None, min_occurrences=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c2bf3d-e2e9-4388-9039-0ef4abf628a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching authors from works: 100%|██| 33/33 [00:24<00:00,  1.33it/s]\n",
      "Building coauthor-work link network: 100%|█| 33/33 [00:00<00:00, 741\n",
      "Fetching works that cite target work: 100%|█| 33/33 [00:12<00:00,  2\n",
      "Fetching authors from works:  89%|▉| 193/218 [02:36<00:21,  1.14it/s/var/folders/98/470nmxrn4mqb47htnln3c5180000gp/T/ipykernel_17025/964968527.py:664: UserWarning: No authorships: work ID https://openalex.org/W4297242708\n",
      "  warnings.warn(f\"No authorships: work ID {work_id}\")\n",
      "Fetching authors from works: 100%|█| 218/218 [02:55<00:00,  1.24it/s\n",
      "Building citing author–work link network: 100%|█| 218/218 [00:00<00:\n"
     ]
    }
   ],
   "source": [
    "target_orcid = orcid_AR\n",
    "# Fetch coauthor and citing author dataframes\n",
    "coauthors_AR = get_coauthors(target_orcid, fields=fields, output='df', df_cols=fields)\n",
    "citers_AR = get_citing_authors(target_orcid, fields=fields, output='df', df_cols=fields)\n",
    "noncoauthors_AR = remove_author_list(citers_AR, coauthors_AR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eaa7ca1-3c84-4b35-9426-d301a36bcf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>occurrences</th>\n",
       "      <th>orcid</th>\n",
       "      <th>summary_stats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/A5071968156</td>\n",
       "      <td>L. V. Posiolova</td>\n",
       "      <td>1</td>\n",
       "      <td>https://orcid.org/0000-0002-1141-0836</td>\n",
       "      <td>{'2yr_mean_citedness': 9.5, 'h_index': 3, 'i10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/A5090166228</td>\n",
       "      <td>Philippe Lognonné</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>{'2yr_mean_citedness': 3.672, 'h_index': 67, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/A5011955426</td>\n",
       "      <td>W. B. Banerdt</td>\n",
       "      <td>5</td>\n",
       "      <td>https://orcid.org/0000-0003-3125-1542</td>\n",
       "      <td>{'2yr_mean_citedness': 4.3, 'h_index': 59, 'i1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/A5014595285</td>\n",
       "      <td>John Clinton</td>\n",
       "      <td>4</td>\n",
       "      <td>https://orcid.org/0000-0001-8626-2703</td>\n",
       "      <td>{'2yr_mean_citedness': 3.8636363636363638, 'h_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/A5088087312</td>\n",
       "      <td>G. S. Collins</td>\n",
       "      <td>20</td>\n",
       "      <td>https://orcid.org/0000-0002-6087-6149</td>\n",
       "      <td>{'2yr_mean_citedness': 9.170731707317072, 'h_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://openalex.org/A5109599438</td>\n",
       "      <td>Constantinos Charalambous</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{'2yr_mean_citedness': 0.0, 'h_index': 3, 'i10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://openalex.org/A5045668185</td>\n",
       "      <td>N. Teanby</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{'2yr_mean_citedness': 0.0, 'h_index': 1, 'i10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://openalex.org/A5000505464</td>\n",
       "      <td>T. Kawamura</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{'2yr_mean_citedness': 0.0, 'h_index': 15, 'i1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://openalex.org/A5039951425</td>\n",
       "      <td>Martin Towner</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{'2yr_mean_citedness': 0.0, 'h_index': 2, 'i10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://openalex.org/A5029774465</td>\n",
       "      <td>Philipe Lognonne</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{'2yr_mean_citedness': 0.0, 'h_index': 0, 'i10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id               display_name  occurrences  \\\n",
       "0    https://openalex.org/A5071968156            L. V. Posiolova            1   \n",
       "1    https://openalex.org/A5090166228          Philippe Lognonné           11   \n",
       "2    https://openalex.org/A5011955426              W. B. Banerdt            5   \n",
       "3    https://openalex.org/A5014595285               John Clinton            4   \n",
       "4    https://openalex.org/A5088087312              G. S. Collins           20   \n",
       "..                                ...                        ...          ...   \n",
       "106  https://openalex.org/A5109599438  Constantinos Charalambous            1   \n",
       "107  https://openalex.org/A5045668185                  N. Teanby            1   \n",
       "108  https://openalex.org/A5000505464                T. Kawamura            1   \n",
       "109  https://openalex.org/A5039951425              Martin Towner            1   \n",
       "110  https://openalex.org/A5029774465           Philipe Lognonne            1   \n",
       "\n",
       "                                     orcid  \\\n",
       "0    https://orcid.org/0000-0002-1141-0836   \n",
       "1                                     None   \n",
       "2    https://orcid.org/0000-0003-3125-1542   \n",
       "3    https://orcid.org/0000-0001-8626-2703   \n",
       "4    https://orcid.org/0000-0002-6087-6149   \n",
       "..                                     ...   \n",
       "106                                   None   \n",
       "107                                   None   \n",
       "108                                   None   \n",
       "109                                   None   \n",
       "110                                   None   \n",
       "\n",
       "                                         summary_stats  \n",
       "0    {'2yr_mean_citedness': 9.5, 'h_index': 3, 'i10...  \n",
       "1    {'2yr_mean_citedness': 3.672, 'h_index': 67, '...  \n",
       "2    {'2yr_mean_citedness': 4.3, 'h_index': 59, 'i1...  \n",
       "3    {'2yr_mean_citedness': 3.8636363636363638, 'h_...  \n",
       "4    {'2yr_mean_citedness': 9.170731707317072, 'h_i...  \n",
       "..                                                 ...  \n",
       "106  {'2yr_mean_citedness': 0.0, 'h_index': 3, 'i10...  \n",
       "107  {'2yr_mean_citedness': 0.0, 'h_index': 1, 'i10...  \n",
       "108  {'2yr_mean_citedness': 0.0, 'h_index': 15, 'i1...  \n",
       "109  {'2yr_mean_citedness': 0.0, 'h_index': 2, 'i10...  \n",
       "110  {'2yr_mean_citedness': 0.0, 'h_index': 0, 'i10...  \n",
       "\n",
       "[111 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coauthors_AR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807dd55e-7897-4d25-bb84-25f0c3b27d33",
   "metadata": {},
   "source": [
    "### Print all citing authors (non-collaborators) ordered by decreasing h-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb37bb0-5004-45fc-94cb-fead29027fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Authors sorted by h-index\n",
       "\n",
       "| Rank | Author                               | h-index | occurrences |\n",
       "| ---- | ------------------------------------ | ------- | ----------- |\n",
       "|    3 | S. M. McLennan                       |     101 |           8 |\n",
       "|    5 | A. S. McEwen                         |     100 |           6 |\n",
       "|   93 | É. Stutzmann                         |      48 |           5 |\n",
       "|  113 | A. Khan                              |      44 |          10 |\n",
       "|  118 | Martín Schimmel                      |      43 |           5 |\n",
       "|  150 | V. Lekić                             |      38 |          12 |\n",
       "|  156 | L. L. Tornabene                      |      37 |           5 |\n",
       "|  163 | ‪Hrvoje Tkalčić                      |      37 |           6 |\n",
       "|  175 | Brigitte Knapmeyer‐Endrun            |      35 |          13 |\n",
       "|  182 | Attilio Rivoldini                    |      34 |           5 |\n",
       "|  197 | Ana‐Catalina Plesa                   |      32 |          10 |\n",
       "|  210 | Martin Knapmeyer                     |      31 |           8 |\n",
       "|  223 | Takuto Maeda                         |      30 |           5 |\n",
       "|  246 | Henri Samuel                         |      28 |           7 |\n",
       "|  268 | P. M. Grindrod                       |      26 |           7 |\n",
       "|  295 | Caroline Beghein                     |      23 |           7 |\n",
       "|  296 | John‐Robert Scholz                   |      23 |           5 |\n",
       "|  374 | Weijia Sun                           |      18 |           5 |\n",
       "|  456 | Jiaqi Li                             |      12 |           9 |\n",
       "|  495 | Sebastián Carrasco                   |       9 |           6 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noncoauthors = copy.deepcopy(noncoauthors_AR)\n",
    "noncoauthors = extract_dict_key_to_column(noncoauthors, dict_col='summary_stats', key='h_index', new_col='h_index', drop_origin=False)\n",
    "\n",
    "# Sort by h-index descending\n",
    "noncoauthors_sorted = noncoauthors.sort_values(by='h_index', ascending=False, ignore_index=True)\n",
    "list_by_sorted_hindex(noncoauthors_sorted, max_count=25, min_occurrences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67636d99-6254-4c11-9c4f-dc76eb9d6b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
